{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hope that my following explanation is helpful, if there is more information I can explain, please let me know via comment or message and I will update the answer.Some Introduction:The \"math\" that goes into it is simple\n",
      "A graph is a list of nodes (vertices) and thee connections (edges).TextRank uses the structure of the text and the known parts of speech for words to assign a score to words that are keywords for the text\n",
      "I don't think it takes much if any math to understand the concepts of the paper, but the terminology may be somewhat dense the first time through\n",
      "The algorithm gives more value to nodes with lots of connections, and gives more influence in steps to better connected nodes, so it reinforces itself and eventually finds its stable score\n",
      "The structure d the text is represented in the way the graph is made.The TextRank Algorithm:First, the words are assigned parts of speech, so that only nouns and adjectives (or some other combination for different applications) are considered\n",
      "Then a graph of words is created\n",
      "The words are the nodes/vertices (denoted V in the paper [1])\n",
      "Each word is connected to other words that are close to it in the text\n",
      "In the graph, this is represented by the connections on the graph (denoted E in the paper for edges [2]).The algorithm is then run on the graph\n",
      "Each node is given a weight of 1\n",
      "Then the algorithm goes through the list of nodes and collects the influence of each of its inbound connections\n",
      "The influence is usually just the value of the connected vertex (initially 1, but it varies) and then summed up to determine the new score for the node\n",
      "Then these scores are normalized, the highest score becomes 1, and the rest are scaled from 0 to 1 based on that value\n",
      "Each time through the algorithm gets closer to the actual \"value\" for each node, and it repeats until the values stop changing.In post-processing, the algorithm takes the top scored words that have been identified as important and outputs them as key/important words\n",
      "They can also be combined if they are used together often.That explanation should be enough to implement the algorithm (you can do a similar thing where you break it up by words or phrases) without needing any more math from the paper.The Rest:The linked \"build your own summary\" project self-identifies as naive, which means it might work, but it doesn't try to use any knowledge, such as word type, to do better, so it makes sense that it doesn't seem to work well.The summary tool uses a different graphical algorithm for determining the important sentences (instead of key words) that functions similarly to Textrank, but it is otherwise different in a few key ways\n",
      "For one, it assigns the strength of the connection by the number of common words in the sentence instead of just being close together in the document\n",
      "Second, it just collects scores once and does not update repeatedly, so that the scores are not updated with new weights for being connected to higher weight sentences\n",
      "Third, it collects the highest value sentence for each paragraph, instead of considering the best sentences from the entire document\n",
      "Finally, because it is naive in nature, it doesn't try to use parts of speech or other language constructs to inform the algorithm in the way that TextRank does.[1] V denotes the collection of all vertices, lowercase v denotes a specific vertex.[2] E denotes all edges/connections, lowercase e denotes a specific edge\n",
      "It is often given subscripts to indicate the \"number\" of the vertex that is being connected from and the vertex that is ring connected to.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = open(\"tt.txt\", \"r\")\n",
    "file2 = open(\"ww.txt\",\"w\")\n",
    "filedata = file.readlines()\n",
    "article = filedata[0].split(\". \")\n",
    "\"\"\"\n",
    "for w in article:\n",
    "    file2.write(w)\n",
    "    file2.write(\"\\n\")\n",
    "file2.close()\n",
    "print(filedata)\n",
    "print(article)\n",
    "print(\"\\n-------------------\\n\")\n",
    "\"\"\"\n",
    "sentences = []\n",
    "for sentence in article:\n",
    "    print(sentence)\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    sentences.append(re.sub('[^a-zA-Z]',' ', sentence).split(' '))\n",
    "sentences.pop()\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bhaskar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sent1=['For', 'years,', 'Facebook', 'gave', 'some', 'of', 'the', \"world's\", 'largest', 'technology', 'companies', 'more', 'intrusive', 'access', 'to', \"users'\", 'personal', 'data', 'than', 'it', 'has', 'disclosed,', 'effectively', 'exempting', 'those', 'business', 'partners', 'from', 'its', 'usual', 'privacy', 'rules,', 'according', 'to', 'internal', 'records', 'and', 'interviews']\n",
    "sent2=['The', 'special', 'arrangements', 'are', 'detailed', 'in', 'hundreds', 'of', 'pages', 'of', 'Facebook', 'documents', 'obtained', 'by', 'The', 'New', 'York', 'Times']\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "if stop_words is None:\n",
    "    stop_words = []\n",
    "sent1 = [w.lower() for w in sent1]\n",
    "sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "all_words = list(set(sent1 + sent2))\n",
    " \n",
    "vector1 = [0] * len(all_words)\n",
    "vector2 = [0] * len(all_words)\n",
    " \n",
    "# build the vector for the first sentence\n",
    "for w in sent1:\n",
    "    if w in stopwords:\n",
    "        continue\n",
    "    vector1[all_words.index(w)] += 1\n",
    " \n",
    "# build the vector for the second sentence\n",
    "for w in sent2:\n",
    "    if w in stopwords:\n",
    "        continue\n",
    "    vector2[all_words.index(w)] += 1\n",
    "print(vector1)\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
